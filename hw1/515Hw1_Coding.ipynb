{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AMATH 515 Homework 1\n",
    "\n",
    "**Due Date: 01/22/2020**\n",
    "\n",
    "*Homework Instruction*: Please follow order of this notebook and fill in the codes where commented as `TODO`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Put your name and UW ID here (for homework grading purposes)\n",
    "UW_ID = \"1772371\"\n",
    "FIRST_NAME = \"Philip\"\n",
    "LAST_NAME = \"Pham\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# require numpy module\n",
    "import numpy as np\n",
    "from numpy.linalg import norm\n",
    "from numpy.linalg import solve\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# import the supplementary function for homework 1\n",
    "import sys\n",
    "sys.path.insert(0, './')\n",
    "from hw1_supp import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient Descent Solver\n",
    "\n",
    "Recall the gradient descent algorithm we learned from the class and complete the gradient descent solver."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def optimizeWithGD(x0, func, grad, step_size, tol=1e-6, max_iter=1000, use_line_search=False):\n",
    "    \"\"\"\n",
    "    Optimize with Gradient Descent\n",
    "    \n",
    "    input\n",
    "    -----\n",
    "    x0 : array_like\n",
    "        Starting point for the solver.\n",
    "    func : function\n",
    "        Input x and return the function value.\n",
    "    grad : function\n",
    "        Input x and return the gradient.\n",
    "    step_size : float or None\n",
    "        If it is a float number and `use_line_search=False`, it will be used as the step size.\n",
    "        Otherwise, line search will be used\n",
    "    tol : float, optional\n",
    "        Gradient tolerance for terminating the solver.\n",
    "    max_iter : int, optional\n",
    "        Maximum number of iteration for terminating the solver.\n",
    "    use_line_search : bool, optional\n",
    "        When it is true a line search will be used, other wise `step_size` has to be provided.\n",
    "        \n",
    "    output\n",
    "    ------\n",
    "    x : array_like\n",
    "        Final solution\n",
    "    obj_his : array_like\n",
    "        Objective function value convergence history\n",
    "    err_his : array_like\n",
    "        Norm of gradient convergence history\n",
    "    exit_flag : int\n",
    "        0, norm of gradient below `tol`\n",
    "        1, exceed maximum number of iteration\n",
    "        2, line search fail\n",
    "        3, others\n",
    "    \"\"\"\n",
    "    # safeguard\n",
    "    if not use_line_search and step_size is None:\n",
    "        print('Please specify the step_size or use the line search.')\n",
    "        return x0, np.array([]), np.array([]), 3\n",
    "    \n",
    "    # initial step\n",
    "    x = np.copy(x0)\n",
    "    g = grad(x)\n",
    "    #\n",
    "    obj = func(x)\n",
    "    err = norm(g)\n",
    "    #\n",
    "    obj_his = np.zeros(max_iter + 1)\n",
    "    err_his = np.zeros(max_iter + 1)\n",
    "    #\n",
    "    obj_his[0] = obj\n",
    "    err_his[0] = err\n",
    "    \n",
    "    # start iterations\n",
    "    iter_count = 0\n",
    "    while err >= tol:\n",
    "        if use_line_search:\n",
    "            step_size = lineSearch(x, g, g, func)\n",
    "        #\n",
    "        # if line search fail step_size will be None\n",
    "        if step_size is None:\n",
    "            print('Gradient descent line search fail.')\n",
    "            return x, obj_his[:iter_count+1], err_his[:iter_count+1], 2\n",
    "        #\n",
    "        # gradient descent step\n",
    "        #####\n",
    "        # TODO: with given step_size, complete gradient descent step\n",
    "        # x = ? \n",
    "        #####\n",
    "        #\n",
    "        # update function and gradient\n",
    "        g = grad(x)\n",
    "        #\n",
    "        obj = func(x)\n",
    "        err = norm(g)\n",
    "        #\n",
    "        iter_count += 1\n",
    "        obj_his[iter_count] = obj\n",
    "        err_his[iter_count] = err\n",
    "        #\n",
    "        # check if exceed maximum number of iteration\n",
    "        if iter_count >= max_iter:\n",
    "            print('Gradient descent reach maximum number of iteration.')\n",
    "            return x, obj_his[:iter_count+1], err_his[:iter_count+1], 1\n",
    "    #\n",
    "    return x, obj_his[:iter_count+1], err_his[:iter_count+1], 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Newton's Solver\n",
    "\n",
    "Recall the Newton's method we learned from the class and complete the Newton's solver.\n",
    "\n",
    "*Remark*: This is a simplified version of Newton's method which do not evolve line search with constant step size 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def optimizeWithNT(x0, func, grad, hess, tol=1e-6, max_iter=100):\n",
    "    \"\"\"\n",
    "    Optimize with Newton's Method\n",
    "    \n",
    "    input\n",
    "    -----\n",
    "    x0 : array_like\n",
    "        Starting point for the solver.\n",
    "    func : function\n",
    "        Input x and return the function value.\n",
    "    grad : function\n",
    "        Input x and return the gradient.\n",
    "    hess : function\n",
    "        Input x and return the Hessian matrix.\n",
    "    tol : float, optional\n",
    "        Gradient tolerance for terminating the solver.\n",
    "    max_iter : int, optional\n",
    "        Maximum number of iteration for terminating the solver.\n",
    "        \n",
    "    output\n",
    "    ------\n",
    "    x : array_like\n",
    "        Final solution\n",
    "    obj_his : array_like\n",
    "        Objective function value convergence history\n",
    "    err_his : array_like\n",
    "        Norm of gradient convergence history\n",
    "    exit_flag : int\n",
    "        0, norm of gradient below `tol`\n",
    "        1, exceed maximum number of iteration\n",
    "        2, others\n",
    "    \"\"\"\n",
    "    # initial step\n",
    "    x = np.copy(x0)\n",
    "    g = grad(x)\n",
    "    H = hess(x)\n",
    "    #\n",
    "    obj = func(x)\n",
    "    err = norm(g)\n",
    "    #\n",
    "    obj_his = np.zeros(max_iter + 1)\n",
    "    err_his = np.zeros(max_iter + 1)\n",
    "    #\n",
    "    obj_his[0] = obj\n",
    "    err_his[0] = err\n",
    "    \n",
    "    # start iteration\n",
    "    iter_count = 0\n",
    "    while err >= tol:\n",
    "        # Newton's step\n",
    "        #####\n",
    "        # TODO: complete Newton's step\n",
    "        # x = ? hint: using solve function\n",
    "        #####\n",
    "        #\n",
    "        # update function, gradient and Hessian\n",
    "        g = grad(x)\n",
    "        H = hess(x)\n",
    "        #\n",
    "        obj = func(x)\n",
    "        err = norm(g)\n",
    "        #\n",
    "        iter_count += 1\n",
    "        obj_his[iter_count] = obj\n",
    "        err_his[iter_count] = err\n",
    "        #\n",
    "        # check if exceed maximum number of iteration\n",
    "        if iter_count >= max_iter:\n",
    "            print('Gradient descent reach maximum number of iteration.')\n",
    "            return x, obj_his[:iter_count+1], err_his[:iter_count+1], 1\n",
    "    #\n",
    "    return x, obj_his[:iter_count+1], err_his[:iter_count+1], 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simple Test\n",
    "\n",
    "Consider a simple test problem,\n",
    "\n",
    "$$\n",
    "\\min_x~~f(x) \\triangleq \\frac{1}{2} \\|x - b\\|^2\n",
    "$$\n",
    "\n",
    "We could easily calculate the gradient and Hessian of $f$,\n",
    "\n",
    "$$\n",
    "\\nabla f(x) = x - b, \\quad \\nabla^2 f(x) = I\n",
    "$$\n",
    "\n",
    "where $I$ is the identity matrix, and we know that $x^* = b$ is the solution.\n",
    "\n",
    "Please complete the solvers above, use the following function as a simple test.\n",
    "\n",
    "*Remark*: Having simple test is always a good idea. It can easily detect the bugs in the code and save a lot of time and emotion energy when use the code in the more complex context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# create b\n",
    "b = np.array([1.0, 2.0, 3.0])\n",
    "# define test function\n",
    "def test_func(x):\n",
    "    return 0.5*sum((x - b)**2)\n",
    "# define test gradient\n",
    "def test_grad(x):\n",
    "    return x - b\n",
    "# define test Hessian\n",
    "def test_hess(x):\n",
    "    return np.eye(b.size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# test gradient descent\n",
    "x0_gd = np.zeros(b.size)\n",
    "#\n",
    "x_gd, obj_his_gd, err_his_gd, exit_flag_gd = optimizeWithGD(x0_gd, test_func, test_grad, 1.0)\n",
    "# check if solution is correct\n",
    "err_gd = norm(x_gd - b)\n",
    "#\n",
    "if err_gd < 1e-6:\n",
    "    print('Gradient Descent: OK')\n",
    "else:\n",
    "    print('Gradient Descent: Err')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# test Newton's method\n",
    "x0_nt = np.zeros(b.size)\n",
    "#\n",
    "x_nt, obj_his_nt, err_his_nt, exit_flag_nt = optimizeWithNT(x0_nt, test_func, test_grad, test_hess)\n",
    "# check if solution is correct\n",
    "err_nt = norm(x_nt - b)\n",
    "#\n",
    "if err_nt < 1e-6:\n",
    "    print('Newton: OK')\n",
    "else:\n",
    "    print('Newont: Err')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression\n",
    "\n",
    "Recall the logistic regression model from the class,\n",
    "$$\n",
    "f(x) = \\sum_{i=1}^m\\left\\{\\log\\left[1+\\exp(\\langle a_i,x\\rangle)\\right]-b_i\\langle a_i,x\\rangle\\right\\} + \\frac{\\lambda}{2}\\|x\\|^2.\n",
    "$$\n",
    "\n",
    "Calculate the gradient and Hessian of the function and define them below using the given data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# fix the random seed to generate same data set\n",
    "np.random.seed(123)\n",
    "# set dimension and create the data\n",
    "m_lgt = 500\n",
    "n_lgt = 50\n",
    "A_lgt = 0.3*np.random.randn(m_lgt, n_lgt)\n",
    "x_lgt = np.random.randn(n_lgt)\n",
    "b_lgt = sampleLGT(x_lgt, A_lgt)\n",
    "lam_lgt = 0.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# define function, gradient and Hessian\n",
    "def lgt_func(x):\n",
    "    #####\n",
    "    # TODO: complete the function\n",
    "    #####\n",
    "    return None\n",
    "#\n",
    "def lgt_grad(x):\n",
    "    #####\n",
    "    # TODO: complete the function\n",
    "    #####\n",
    "    return None\n",
    "#\n",
    "def lgt_hess(x):\n",
    "    #####\n",
    "    # TODO: complete the function\n",
    "    #####\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# uniform step size for gradient descent or should we use line search?\n",
    "#####\n",
    "# TODO: if there is a uniform stepsize, set up step_size_lgt = \n",
    "# otherwise set use_line_search=True\n",
    "#####"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Apply gradient descent solver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x0_lgt_gd = np.zeros(n_lgt)\n",
    "x_lgt_gd, obj_his_lgt_gd, err_his_lgt_gd, exit_flag_lgt_gd = \\\n",
    "    optimizeWithGD(x0_lgt_gd, lgt_func, lgt_grad, step_size_lgt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# plot result\n",
    "fig, ax = plt.subplots(1, 2, figsize=(12,5))\n",
    "ax[0].plot(obj_his_lgt_gd)\n",
    "ax[0].set_title('function value')\n",
    "ax[1].semilogy(err_his_lgt_gd)\n",
    "ax[1].set_title('norm of gradient')\n",
    "fig.suptitle('Gradient Descent on Logistic Regression')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Apply Newton's solver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x0_lgt_nt = np.zeros(n_lgt)\n",
    "x_lgt_nt, obj_his_lgt_nt, err_his_lgt_nt, exit_flag_lgt_nt = \\\n",
    "    optimizeWithNT(x0_lgt_nt, lgt_func, lgt_grad, lgt_hess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# plot result\n",
    "fig, ax = plt.subplots(1, 2, figsize=(12,5))\n",
    "ax[0].plot(obj_his_lgt_nt)\n",
    "ax[0].set_title('function value')\n",
    "ax[1].semilogy(err_his_lgt_nt)\n",
    "ax[1].set_title('norm of gradient')\n",
    "fig.suptitle('Newton\\'s Method on Logistic Regression')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Poisson Regression\n",
    "\n",
    "Recall the logistic regression model from the class,\n",
    "$$\n",
    "f(x)=\\sum_{i=1}^m\\left[\\exp(\\langle a_i,x\\rangle)-b_i\\langle a_i,x\\rangle\\right]+\\frac{\\lambda}{2}\\|x\\|^2\n",
    "$$\n",
    "\n",
    "Calculate the gradient and Hessian of the function and define them below using the given data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# fix the random seed to generate same data set\n",
    "np.random.seed(123)\n",
    "# set dimension and create the data\n",
    "m_psn = 500\n",
    "n_psn = 50\n",
    "A_psn = 0.3*np.random.randn(m_psn, n_psn)\n",
    "x_psn = np.random.randn(n_psn)\n",
    "b_psn = samplePSN(x_psn, A_psn)\n",
    "lam_psn = 0.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# define function, gradient and Hessian\n",
    "def psn_func(x):\n",
    "    #####\n",
    "    # TODO: complete the function\n",
    "    #####\n",
    "    return None\n",
    "#\n",
    "def psn_grad(x):\n",
    "    #####\n",
    "    # TODO: complete the function\n",
    "    #####\n",
    "    return None\n",
    "#\n",
    "def psn_hess(x):\n",
    "    #####\n",
    "    # TODO: complete the function\n",
    "    #####\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# uniform step size for gradient descent or should we use line search?\n",
    "#####\n",
    "# TODO: if there is a uniform stepsize, set up step_size_lgt = \n",
    "# otherwise set use_line_search=True\n",
    "#####"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Apply gradient descent solver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x0_psn_gd = np.zeros(n_lgt)\n",
    "x_psn_gd, obj_his_psn_gd, err_his_psn_gd, exit_flag_psn_gd = \\\n",
    "    optimizeWithGD(x0_psn_gd, psn_func, psn_grad, None, use_line_search=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# plot result\n",
    "fig, ax = plt.subplots(1, 2, figsize=(12,5))\n",
    "ax[0].plot(obj_his_psn_gd)\n",
    "ax[0].set_title('function value')\n",
    "ax[1].semilogy(err_his_psn_gd)\n",
    "ax[1].set_title('norm of gradient')\n",
    "fig.suptitle('Gradient Descent on Poisson Regression')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Apply Newton's solver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x0_psn_nt = np.zeros(n_lgt)\n",
    "x_psn_nt, obj_his_psn_nt, err_his_psn_nt, exit_flag_psn_nt = \\\n",
    "    optimizeWithNT(x0_psn_nt, psn_func, psn_grad, psn_hess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# plot result\n",
    "fig, ax = plt.subplots(1, 2, figsize=(12,5))\n",
    "ax[0].plot(obj_his_psn_nt)\n",
    "ax[0].set_title('function value')\n",
    "ax[1].semilogy(err_his_psn_nt)\n",
    "ax[1].set_title('norm of gradient')\n",
    "fig.suptitle('Newton\\'s Method on Poisson Regression')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
