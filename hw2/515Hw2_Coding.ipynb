{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AMATH 515 Homework 2\n",
    "\n",
    "**Due Date: 02/19/2020**\n",
    "\n",
    "*Homework Instruction*: Please follow order of this notebook and fill in the codes where commented as `TODO`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Put your name and UW ID here (for homework grading purposes)\n",
    "UW_ID = \"\"\n",
    "FIRST_NAME = \"\"\n",
    "LAST_NAME = \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy.io as sio\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Please complete the solvers in `solver.py`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('./')\n",
    "from solvers import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 3: Compressive Sensing\n",
    "\n",
    "Consier the optimization problem,\n",
    "\n",
    "$$\n",
    "\\min_x~~\\frac{1}{2}\\|Ax - b\\|^2 + \\lambda\\|x\\|_1\n",
    "$$\n",
    "\n",
    "In the following, please specify the $f$ and $g$ and use the proximal gradient descent solver to obtain the solution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the data\n",
    "np.random.seed(123)\n",
    "m = 100  # number of measurements\n",
    "n = 500  # number of variables\n",
    "k = 10   # number of nonzero variables\n",
    "s = 0.05 # measurements noise level\n",
    "#\n",
    "A_cs = np.random.randn(m, n)\n",
    "x_cs = np.zeros(n)\n",
    "x_cs[np.random.choice(range(n), k, replace=False)] = np.random.choice([-1.0, 1.0], k)\n",
    "b_cs = A_cs.dot(x_cs) + s*np.random.randn(m)\n",
    "#\n",
    "lam_cs = 0.1*norm(A_cs.T.dot(b_cs), np.inf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the function, prox and the beta constant\n",
    "def func_f_cs(x):\n",
    "    # TODO: complete the function\n",
    "\n",
    "def func_g_cs(x):\n",
    "    # TODO: complete the gradient\n",
    "\n",
    "def grad_f_cs(x):\n",
    "    # TODO: complete the function\n",
    "\n",
    "def prox_g_cs(x, t):\n",
    "    # TODO: complete the prox of 1 norm\n",
    "\n",
    "# TODO: what is the beta value for the smooth part\n",
    "beta_f_cs = ???"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Proximal gradient descent on compressive sensing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply the proximal gradient descent solver\n",
    "x0_cs_pgd = np.zeros(x_cs.size)\n",
    "x_cs_pgd, obj_his_cs_pgd, err_his_cs_pgd, exit_flag_cs_pgd = \\\n",
    "    optimizeWithPGD(x0_cs_pgd, func_f_cs, func_g_cs, grad_f_cs, prox_g_cs, beta_f_cs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot signal result\n",
    "plt.plot(x_cs)\n",
    "plt.plot(x_cs_pgd, '.')\n",
    "plt.legend(['true signal', 'recovered'])\n",
    "plt.title('Compressive Sensing Signal')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot result\n",
    "fig, ax = plt.subplots(1, 2, figsize=(12,5))\n",
    "ax[0].plot(obj_his_cs_pgd)\n",
    "ax[0].set_title('function value')\n",
    "ax[1].semilogy(err_his_cs_pgd)\n",
    "ax[1].set_title('optimality condition')\n",
    "fig.suptitle('Proximal Gradient Descent on Compressive Sensing')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Accelerate proximal gradient descent on compressive sensing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply the proximal gradient descent solver\n",
    "x0_cs_apgd = np.zeros(x_cs.size)\n",
    "x_cs_apgd, obj_his_cs_apgd, err_his_cs_apgd, exit_flag_cs_apgd = \\\n",
    "    optimizeWithAPGD(x0_cs_apgd, func_f_cs, func_g_cs, grad_f_cs, prox_g_cs, beta_f_cs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot signal result\n",
    "plt.plot(x_cs)\n",
    "plt.plot(x_cs_pgd, '.')\n",
    "plt.legend(['true signal', 'recovered'])\n",
    "plt.title('Compressive Sensing Signal')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot result\n",
    "fig, ax = plt.subplots(1, 2, figsize=(12,5))\n",
    "ax[0].plot(obj_his_cs_apgd)\n",
    "ax[0].set_title('function value')\n",
    "ax[1].semilogy(err_his_cs_apgd)\n",
    "ax[1].set_title('optimality condition')\n",
    "fig.suptitle('Accelerated Proximal Gradient Descent on Compressive Sensing')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 4: Logistic Regression on MINST Data\n",
    "\n",
    "Now let's play with some real data, recall the logistic regression problem,\n",
    "\n",
    "$$\n",
    "\\min_x~~\\sum_{i=1}^m\\left\\{\\log(1 + \\exp(\\langle a_i,x \\rangle)) - b_i\\langle a_i,x \\rangle\\right\\} + \\frac{\\lambda}{2}\\|x\\|^2.\n",
    "$$\n",
    "\n",
    "Here our data pair $\\{a_i, b_i\\}$, $a_i$ is the image and $b_i$ is the label.\n",
    "In this homework problem, let's consider the binary classification problem, where $b_i \\in \\{0, 1\\}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import data\n",
    "mnist_data = np.load('mnist01.npy')\n",
    "#\n",
    "A_lgt = mnist_data[0]\n",
    "b_lgt = mnist_data[1]\n",
    "A_lgt_test = mnist_data[2]\n",
    "b_lgt_test = mnist_data[3]\n",
    "#\n",
    "# set regularizer parameter\n",
    "lam_lgt = 0.1\n",
    "#\n",
    "# beta constant of the function\n",
    "beta_lgt = 0.25*norm(A_lgt, 2)**2 + lam_lgt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the images\n",
    "fig, ax = plt.subplots(1, 2)\n",
    "ax[0].imshow(A_lgt[0].reshape(28,28))\n",
    "ax[1].imshow(A_lgt[7].reshape(28,28))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define function, gradient and Hessian\n",
    "def lgt_func(x):\n",
    "    # TODO: complete the function of logistic regression\n",
    "#\n",
    "def lgt_grad(x):\n",
    "    # TODO: complete the gradient of logistic regression\n",
    "#\n",
    "def lgt_hess(x):\n",
    "    # TODO: complete the hessian of logistic regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient decsent on logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply the gradient descent\n",
    "x0_lgt_gd = np.zeros(A_lgt.shape[1])\n",
    "x_lgt_gd, obj_his_lgt_gd, err_his_lgt_gd, exit_flag_lgt_gd = \\\n",
    "    optimizeWithGD(x0_lgt_gd, lgt_func, lgt_grad, beta_lgt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot result\n",
    "fig, ax = plt.subplots(1, 2, figsize=(12,5))\n",
    "ax[0].plot(obj_his_lgt_gd)\n",
    "ax[0].set_title('function value')\n",
    "ax[1].semilogy(err_his_lgt_gd)\n",
    "ax[1].set_title('optimality condition')\n",
    "fig.suptitle('Gradient Descent on Logistic Regression')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Accelerate Gradient decsent on logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply the accelerated gradient descent\n",
    "x0_lgt_agd = np.zeros(A_lgt.shape[1])\n",
    "x_lgt_agd, obj_his_lgt_agd, err_his_lgt_agd, exit_flag_lgt_agd = \\\n",
    "    optimizeWithAGD(x0_lgt_agd, lgt_func, lgt_grad, beta_lgt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot result\n",
    "fig, ax = plt.subplots(1, 2, figsize=(12,5))\n",
    "ax[0].plot(obj_his_lgt_agd)\n",
    "ax[0].set_title('function value')\n",
    "ax[1].semilogy(err_his_lgt_agd)\n",
    "ax[1].set_title('optimality condition')\n",
    "fig.suptitle('Accelerated Gradient Descent on Logistic Regression')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Accelerate Gradient decsent on logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply the accelerated gradient descent\n",
    "x0_lgt_nt = np.zeros(A_lgt.shape[1])\n",
    "x_lgt_nt, obj_his_lgt_nt, err_his_lgt_nt, exit_flag_lgt_nt = \\\n",
    "    optimizeWithNT(x0_lgt_nt, lgt_func, lgt_grad, lgt_hess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot result\n",
    "fig, ax = plt.subplots(1, 2, figsize=(12,5))\n",
    "ax[0].plot(obj_his_lgt_nt)\n",
    "ax[0].set_title('function value')\n",
    "ax[1].semilogy(err_his_lgt_nt)\n",
    "ax[1].set_title('optimality condition')\n",
    "fig.suptitle('Newton\\'s Method on Logistic Regression')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define accuracy function\n",
    "def accuracy(x, A_test, b_test):\n",
    "    r = A_test.dot(x)\n",
    "    b_test[b_test == 0.0] = -1.0\n",
    "    correct_count = np.sum((r*b_test) > 0.0)\n",
    "    return correct_count/b_test.size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('accuracy of the result is %0.3f' % accuracy(x_lgt_nt, A_lgt_test, b_lgt_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
